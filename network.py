
from typing import List, Dict, NamedTuple, Tuple
from gamecomponents import Action, Policy, RandomAction
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import Model
from categorised_scalar import categorise, decategorise
from helpers import TFMinMaxStats


## Container for the parameters generated by the network object
class NetworkOutput(NamedTuple): 
    value: float
    reward: float
    policy_logits: Policy
    hidden_state: List[float]


M = 128

## basic scalar loss - defined external the methods to allow 
# different functions to be applied in different domains
@tf.function
def scalar_loss(prediction, target, mask = True):

    target = categorise(target)

    return tf.nn.softmax_cross_entropy_with_logits(logits=prediction,
                                                    labels=target)
    #return tf.keras.losses.MSE(x,y)



@tf.function
def scale_gradient(tensor, scale):
    return tensor * scale + tf.stop_gradient(tensor) * (1 - scale)

## key object 
class Network():
    
    action_count = [] # number of actions (configures prediction network output)
    N = 4 # hidden state size
    InSize = 8
    grad_scale = (1.,1.,1.) # allows us to balance the different loss components if required

    def __init__(self):
        
        # we choose to have the optimiser attached to the network object
        learning_rate = 0.00001 #config.lr_init * config.lr_decay_rate**(tf.train.get_global_step() / config.lr_decay_steps)
        self.optimiser = tf.keras.optimizers.SGD(learning_rate, 0.9)
        self.minmax = TFMinMaxStats(Network.N)
        
        self.count = 0

        
        # there are three sub-networks each providing a different service
        # the key I/O is:
        # * observation of the external environment
        # * internal (abstract) representation of the system state
        # * predicted value of the current state
        # * estimated priors for the policy in the current state


        # representation network
        # - given an external observation, what is the corresponding internal state representation
        observation = layers.Input([Network.InSize])
        y1 =layers.Dense(M)(observation)
        y2 =layers.Activation('relu')(y1)
        y3 =layers.Dense(M)(y2)
        y4 =layers.Activation('relu')(y3)
        hidden_state = layers.Dense(Network.N)(y4)
        
        self.representation = Model(observation, hidden_state)


        # prediction network
        # - given the internal state representation, what is the estimated policy and value
        hstate = layers.Input([Network.N])
        x1 =layers.Dense(M)(hstate)
        x2 =layers.Activation('relu')(x1)
        x3 =layers.Dense(M)(x2)
        x4 =layers.Activation('relu')(x3)
        x5 =layers.Dense(M)(x4)
        x6 =layers.Activation('relu')(x5)
        policy = layers.Dense(Network.action_count, kernel_initializer = tf.keras.initializers.Constant(value=1.0),
                              use_bias = False, name = 'policy_op')(x6) 
        x7 =layers.Dense(M)(x4)
        x8 =layers.Activation('relu')(x7)
        value = layers.Dense(600)(x8)

        self.prediction = Model(hstate, [policy, value])



        # dynamics network
        # - given the internal state representation and an action, 
        # what is the estimated reward and the next state 
        hstate = layers.Input([Network.N])
        action = layers.Input([1])
        hstate_action = layers.concatenate([hstate, action])
        z1 =layers.Dense(M)(hstate_action)
        z2 =layers.Activation('relu')(z1)
        z3 =layers.Dense(M)(z2)
        z4 =layers.Activation('relu')(z3)
        z5 =layers.Dense(M)(z4)
        z6 =layers.Activation('relu')(z5)
        reward = layers.Dense(600)(z6)

        z7 =layers.Dense(M)(z4)
        z8 =layers.Activation('relu')(z7)
        new_state = layers.Dense(Network.N, name = 'dynamics_state_op')(z8)

        self.dynamics = Model([hstate, action], [reward, new_state])


        # collect all the trainable parameters
        self.vars = self.dynamics.trainable_weights + self.representation.trainable_weights + self.prediction.trainable_weights
        

    ##
    # Initial inference functions, which take the current observation and generate 
    # estmates for all the key parameters

    # wrapper for the outside world, assuming single inputs and outputs plus strict typing
    def initial_inference(self, image) -> NetworkOutput: 
   
        # image (4,)

        image = tf.convert_to_tensor(image, tf.float32)
        image = tf.reshape(image,[-1,Network.InSize])
        
        value, reward, policy_logits, hidden_state = self._initial_inference(image)
        # value (None,1), reward [], policy_logits [], hidden_state (None, N)
        
        value = decategorise(tf.nn.softmax(value))
        reward = decategorise(tf.nn.softmax(reward))
        return NetworkOutput(value[0].numpy(), 
                            reward[0].numpy(),  
                            Policy(policy_logits[0].numpy()), 
                            hidden_state.numpy())

    # internal method aimed to streamline the training process
    #@tf.function
    def _initial_inference(self, image) -> Tuple: 
        # image (None, 4)    
        
        hidden = self.representation(image)
        # hidden (None, N)
        policy_logits, value = self.prediction(hidden)
        # policy_logits (None,action_count), value (None,1) 
        #policy_logits = tf.divide(policy_logits, tf.reduce_sum(policy_logits,axis = 1, keepdims= True))
        return (value, tf.zeros(value.shape), policy_logits, hidden)
    
    ##
    # Initial inference functions, which take the current state representation, 
    # plus an action and generate estmates for all the key parameters
    
    # wrapper for the outside world, assuming single inputs and strict typing
    def recurrent_inference(self, hidden_state, action) -> NetworkOutput: 
       
        action = tf.convert_to_tensor([action.__hash__()], tf.float32)
        action= tf.reshape(action,[-1, 1])
        # action (None,1)
        
        hidden_state= tf.reshape(hidden_state,[-1, Network.N])
        # hidden_state (None,N)
        
        value, reward, policy_logits, hidden_state = self._recurrent_inference(hidden_state, action)
        
        value = decategorise(tf.nn.softmax(value))
        reward = decategorise(tf.nn.softmax(reward))
        return NetworkOutput(value[0].numpy(), 
                            reward[0].numpy(), 
                            Policy(policy_logits[0].numpy()), 
                            hidden_state.numpy())
    
    # internal method aimed to streamline the training process
    #@tf.function
    def _recurrent_inference(self, hidden_state, action) -> Tuple: 
        # hidden_state (None,N), action (None,1)
      
        hidden_state = self.minmax.normalize(hidden_state)
        
        reward, hidden = self.dynamics([hidden_state, action])
        # reward (None,1), hidden (None,N) 
        policy_logits, value = self.prediction(hidden)
        # policy_logits (None,action_count), value (None,1) 

        #policy_logits = tf.divide(policy_logits, tf.reduce_sum(policy_logits,axis = 1, keepdims= True))
        return (value, reward, policy_logits, hidden)        
    
    ## 

    def get_weights(self):
        # Returns the weights of this network. 
        return []
    
    def training_steps(self) -> int:
        # How many steps / batches the network has been trained for. 
        return self.count
    
    
    ##
    # training
    
    def update_weights(self, batch, weight_decay: float, hidden_state_dampen: float = 0.5):
        loss = 0
        vLoss = 0
        rLoss = 0
        pLoss = 0
        batch_size = len(batch)
        
        self.count += 1
        # first, find the data from the games - some assembly required
        images, actions, targets = zip(*batch)
        
        # make a tensor from the images
        images = tf.cast(tf.stack(list(images)), tf.float32)
        
        # preprocess the actions
        actions = list(actions)
        num_actions = tf.convert_to_tensor([len(a) for a in actions], dtype=tf.float32)
        scaling = 1./num_actions
        actions = [[tf.cast(j.__hash__(), tf.float32) for j in k] for k in actions]
        pad = RandomAction()
        actions = tf.keras.preprocessing.sequence.pad_sequences(actions, 
                                                                dtype='float32', 
                                                                padding='post', 
                                                                value=pad.__hash__())
        # actions (None, num_unroll_steps)

        targets = [list(x) for x in zip(*targets)]
        
        with tf.GradientTape() as g:
        
            ## Calculate the network predictions

            # Initial step, starting from the real observation.
            value, reward, policy_logits, hidden_state = self._initial_inference(images)
            predictions = [(1.0, value, reward, policy_logits)]
    
            self.minmax.update(hidden_state)
            
            # Recurrent steps, starting from previous hidden state and the action 
            for a in range(actions.shape[1]):
                action = actions[:,a:a+1]
                # action (None,1)
                value, reward, policy_logits, hidden_state = self._recurrent_inference(hidden_state, action)
                # value (None,1), reward (None,1), policy_logits (None,action_count), hidden_state (None,N) 
                
                predictions.append((scaling, value, reward, policy_logits))
                
                # the further away from the starting point, the less 
                # confident we are in the hidden state
                self.minmax.update(hidden_state)
                hidden_state = scale_gradient(hidden_state, hidden_state_dampen)
                
                
            ## Assemble training data & create the losses

            for prediction, target in zip(predictions, targets): 
                
                gradient_scale, value, reward, policy_logits = prediction 
                
                # preprocess the targets
                target_value, target_reward, target_policy, target_mask = zip(*target)
                
                target_policy = tf.convert_to_tensor(target_policy)
                #target_policy = tf.divide(target_policy, tf.reduce_sum(target_policy, axis = 1, keepdims= True)+0.001)
        
                target_value = tf.convert_to_tensor(target_value)

                
                target_reward = tf.convert_to_tensor(target_reward)

                target_mask = tf.convert_to_tensor(target_mask)

                
                # calculate the losses. 
                # (Do this individually, to allow us to inspect them separately.)
                policy_loss = tf.nn.softmax_cross_entropy_with_logits(logits=policy_logits,
                                                                      labels=target_policy)
                #pLoss += tf.reduce_mean(scale_gradient(policy_loss, gradient_scale))
                pLoss += tf.reduce_mean(tf.boolean_mask(policy_loss*gradient_scale, target_mask))

                value_loss = scalar_loss(prediction = value, target =  target_value)
                #vLoss += tf.reduce_mean(scale_gradient(value_loss, gradient_scale))
                vLoss += tf.reduce_mean(tf.boolean_mask(value_loss*gradient_scale, target_mask))


                reward_loss = scalar_loss(prediction = reward, target =  target_reward) 
                #rLoss += tf.reduce_mean(scale_gradient(reward_loss, gradient_scale))

                rLoss += tf.reduce_mean(tf.boolean_mask(reward_loss*gradient_scale, target_mask))    
                

                
            # combine the losses     
            loss = (Network.grad_scale[0]*vLoss + Network.grad_scale[1]*rLoss + Network.grad_scale[2]*pLoss)
            #loss = (Network.grad_scale[2]*pLoss)

            for weights in self.vars:
                loss += weight_decay * tf.nn.l2_loss(weights)
    
        grads = g.gradient(loss, self.vars)

        self.optimiser.apply_gradients(zip(grads, self.vars))

        return (vLoss.numpy(), rLoss.numpy(), pLoss.numpy())



# network storage
class SharedStorage(object):
    def __init__(self): 
        self._networks = {}

    def latest_network(self) -> Network: 
        if self._networks:
            return self._networks[max(self._networks.keys())] 
        else:
            # Paper: policy -> uniform, value -> 0, reward -> 0
            # return make_uniform_network() 
            # network is designed with these defaults, so just return one
            return Network()
        
    def save_network(self, step: int, network: Network): 
        self._networks[step] = network

    

